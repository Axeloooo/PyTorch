{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAIcLwfa0Lqcnf8gTfQYdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Axeloooo/PyTorch/blob/main/02_pytorch_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. PyTorch Neural Network Classification\n",
        "\n",
        "Classification is a problem of predicting wheather something is one thing or another (there can be multiple things as an option)"
      ],
      "metadata": {
        "id": "0cyUynsTlvca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Make classification data and get it ready"
      ],
      "metadata": {
        "id": "8nm4fNcXmDVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "m_F1ewPJl0pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "#Make 1000 sample\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples,\n",
        "                    noise=0.03,\n",
        "                    random_state=42)"
      ],
      "metadata": {
        "id": "orfQftX0mIpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "nzrsF8aBmxwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 5 samples of X:\\n {X[:5]}\")\n",
        "print(f\"First 5 samples of y:\\n {y[:5]}\")"
      ],
      "metadata": {
        "id": "fyazEW8mmz4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a Dataframe of cirlce data\n",
        "import pandas as pd\n",
        "circles = pd.DataFrame({\"X1\": X[:, 0],\n",
        "                        \"X2\": X[:, 1],\n",
        "                        \"label\": y})\n",
        "circles.head(10)"
      ],
      "metadata": {
        "id": "qoPqVdN4nUUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:, 0],\n",
        "            y=X[:, 1],\n",
        "            c=y,\n",
        "            cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "YUPT5BADnpWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The data we're working with is often referred to as a toty dataset, taht is small enough to experiment but still sizeable enough to practice the fundamentals."
      ],
      "metadata": {
        "id": "xIRBwdXBoHQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Check input and output shapes"
      ],
      "metadata": {
        "id": "nGaeCcMCw_Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "LlKHlMXDoZ2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first example of features and labels\n",
        "X_sample = X[0]\n",
        "y_sample = y[0]\n",
        "\n",
        "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
        "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
      ],
      "metadata": {
        "id": "olJYXsWjyKel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Turn data into tensors and create train and test splits"
      ],
      "metadata": {
        "id": "wgPgP_Gcyi-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn data into tensors\n",
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "I1MyTajHyn1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float32)\n",
        "y = torch.from_numpy(y).type(torch.float32)\n",
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "5OPmkZmDyuNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "DNgcMeDKzNLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "iYj6s_PO0Ir_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Building a model\n",
        "\n",
        "Let's build a model to clasify our blue and red dots.\n",
        "\n",
        "To do so, we want to:\n",
        "1. Setup device agnostic code so our code will run on an accelerator (GPU) if there is one\n",
        "2. Construct a model (by subclassing `nn.Module`)\n",
        "3. Define a loss function and optimizer\n",
        "4. Create a training and test loop"
      ],
      "metadata": {
        "id": "d7Kwect91O51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nn\n",
        "from torch import nn\n",
        "\n",
        "# Make device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "lApUr9Rq1lT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've setup device agnostic code, let's create a model that:\n",
        "1. Subclasses `nn.Module` (almost all models in PyTorch subclass `nn.Module`)\n",
        "2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data\n",
        "3. Defines a `forward()` method that outlines the forward pass (or forward computation) of the model\n",
        "4. Instantiate an instance of our model class and send it to the target `device`"
      ],
      "metadata": {
        "id": "Ap-g6q-l2XFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Construct a mode that subclasses nn.Module\n",
        "class CircleModelV0(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 2. Create a 2 nn.Layer() layers capable of handling the shapes of our data\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 8 features\n",
        "    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature\n",
        "\n",
        "  # 3. Define a forward() method that outlines the forward pass\n",
        "  def forward(self, x):\n",
        "    return self.layer_2(self.layer_1(x)) # x -> layer_1 -> layer_2 -> output\n",
        "\n",
        "# 4. Instantiate an instance of our model class and send it to the target device\n",
        "model_0 = CircleModelV0().to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "Fj-B1UlU2acv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device"
      ],
      "metadata": {
        "id": "VLLMEp5E5jg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the model above using nn.Sequential\n",
        "model_0 = nn.Sequential(\n",
        "    nn.Linear(in_features=2, out_features=5),\n",
        "    nn.Linear(in_features=5, out_features=1)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "xeegPw1AaOey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "IhLd1mISbaLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.inference_mode():\n",
        "  untrained_preds = model_0(X_test.to(device))\n",
        "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
        "print(f\"Length of test sample: {len(X_test)}, Shape: {X_test.shape}\")\n",
        "print(f\"First 10 predictions: {untrained_preds[:10]}\")\n",
        "print(f\"First 1o labels: {y_test[:10]}\")"
      ],
      "metadata": {
        "id": "24uvQabYbxoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Setup loss function and optimizer\n",
        "\n",
        "For `regression` you might want MAE or MSE (mean absolute error or mean squared error).\n",
        "\n",
        "For `classification` you might want binary cross entropy or categorcial cross entropy.\n",
        "\n",
        "As a reminder the loss function measures how wrong your model's predictions are.\n",
        "\n",
        "And for optimizers, two of the most common and useful are SDG and Adam, however PyTorch has many built-in options.\n"
      ],
      "metadata": {
        "id": "sMEUeC09dTmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss function\n",
        "# loss_fn = nn.BCELoss() # requires inputs to have gone through the sigmoid activation function prior to input to BCELoss\n",
        "loss_fn = nn.BCEWithLogitsLoss() # sigmoid activation function built-in\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "USFqmVlldW8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy - out of a 100 example, what percetnage does our model get right\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "dPFjvU1ygE0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "\n",
        "To train a model, we need to build a trainig loop:\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward (backpropagation)\n",
        "5. Optimizer step (gradient descent)"
      ],
      "metadata": {
        "id": "0Oj74e9Hg49w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Going from raw logits -> predictions probabilities -> prediction labels\n",
        "\n",
        "Our model outputs are going to be raw **logits**.\n",
        "\n",
        "We can convert these **logits** into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary classification and softmax for multiclass classification)\n",
        "\n",
        "Then we can convert our model's prediction probabilities to **predictions labels** by either roundinbg them or taking the `argmax()`"
      ],
      "metadata": {
        "id": "5GcZ7_l6hUok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 outputs of the forward pass on the test data\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_0(X_test.to(device))[:5]\n",
        "y_logits"
      ],
      "metadata": {
        "id": "_btO55-XhR0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:5]"
      ],
      "metadata": {
        "id": "-XWnmdGFipMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n",
        "y_pred_probs = torch.sigmoid(y_logits)\n",
        "y_pred_probs"
      ],
      "metadata": {
        "id": "ebrn6g0Wittw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our prediction probabilities values we need to form a range-style rounding on them:\n",
        "\n",
        "* `y_pred_probs` >= 0.5, `y=1` (class 1)\n",
        "* `y_pred_probs` < 0.5, `y=0` (class 0)"
      ],
      "metadata": {
        "id": "NrsWm3AHjGSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted labels\n",
        "y_preds =  torch.round(y_pred_probs)\n",
        "\n",
        "# In full (logits -> pred probs -> pred labels)\n",
        "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "acWrrSP6i9oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Buidling a training and testing loop"
      ],
      "metadata": {
        "id": "iDMRCAuolL7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Building training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_0(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "\n",
        "  # 2. Calculate loss/accuracy\n",
        "  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects predictions probabilities as input\n",
        "  #                y_train)\n",
        "  loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
        "                 y_train)\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step (gradient descent)\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_0(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    # 2. Calculate test loss/acc\n",
        "    test_loss = loss_fn(test_logits,\n",
        "                        y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test,\n",
        "                           y_pred=test_pred)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "CS-AWHiplPCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and evaluate the model\n",
        "\n",
        "From the metrics looks like our model isn't learning anything."
      ],
      "metadata": {
        "id": "LobkWUdSv-cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if it's not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ],
      "metadata": {
        "id": "4mnqEG6SwL6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary of the model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_0, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_0, X_test, y_test)"
      ],
      "metadata": {
        "id": "nxYxIpVv1HW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. improving a model (from a model perspective)\n",
        "\n",
        "* Add more layer - give the model more chances to learn about poatterns in the data\n",
        "* Add more hidden units - go from 5 hidden units to 10 hidden units\n",
        "* Fit for longer\n",
        "* Changing the activation functions\n",
        "* Change the learning rate\n",
        "* Change the loss function\n",
        "\n",
        "These options are all from a model's perspective because they deal directly with the model, rather than the data.\n",
        "\n",
        "And because these options are all values we can change, they are referred as **hyperparameters**.\n",
        "\n",
        "Let's try and improve our model by:\n",
        "* Adding more hidden units: 5 -> 10\n",
        "* Increase the number of layers: 2 -> 3\n",
        "* Increase the numbers of epochs: 100 -> 1000"
      ],
      "metadata": {
        "id": "s-wq6gqJ3mzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # z = self.layer_1(x)\n",
        "    # z = self.layer_2(z)\n",
        "    # z = self.layer_3(z)\n",
        "    return self.layer_3(self.layer_2(self.layer_1(x))) # this way of writing operations leverages speed ups where\n",
        "\n",
        "model_1 = CircleModelV1().to(device)\n",
        "model_1"
      ],
      "metadata": {
        "id": "mp96YKdZ3qmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "id": "_bGMoMF79VHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "33c9SsUl87eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a training and evaluation loop fpr model_1\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Train for longer\n",
        "epochs = 1000\n",
        "\n",
        "# Put data on the target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_1.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_1(X_train).squeeze()\n",
        "  y_preds = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "  # 2. Calculate the loss/acc\n",
        "  loss = loss_fn(y_logits, y_train)\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_preds)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. optimizer step (gradient descent)\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits = model_1(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    # 2. Calculate test loss/acc\n",
        "    test_loss = loss_fn(test_logits,\n",
        "                        y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test,\n",
        "                           y_pred=test_pred)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "QYo5tzJs9nfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary of the model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_1, X_test, y_test)"
      ],
      "metadata": {
        "id": "lJaOps2u_e3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Preparing data to see if our model can fit a straight line\n",
        "One way to troubleshoot to a larger problem is to test out a smaller problem.\n"
      ],
      "metadata": {
        "id": "X0-JOrIGAKhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data (same as notebook 01)\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.01\n",
        "\n",
        "# Create data\n",
        "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y_regression = weight * X_regression + bias # Linear regression formula (without epsilon)\n",
        "\n",
        "# Check data\n",
        "print(len(X_regression))\n",
        "X_regression[:5], y_regression[:5]"
      ],
      "metadata": {
        "id": "quRSznJYAVLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test splits\n",
        "train_split = int(0.8 * len(X_regression))\n",
        "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
        "\n",
        "# Check the lengths of each\n",
        "len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"
      ],
      "metadata": {
        "id": "ERCwQlzbA1K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels=y_train_regression,\n",
        "                 test_data=X_test_regression,\n",
        "                 test_labels=y_test_regression)"
      ],
      "metadata": {
        "id": "-W2jZv2FBawH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Adjusting `model_1` to fit a straight line"
      ],
      "metadata": {
        "id": "3diz6ZElB01B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same architecture as model 1 (but using nn.Sequential())\n",
        "model_2 = nn.Sequential(\n",
        "    nn.Linear(in_features=1, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=10),\n",
        "    nn.Linear(in_features=10, out_features=1)\n",
        ").to(device)\n",
        "\n",
        "model_2"
      ],
      "metadata": {
        "id": "mRZ_i-MXCCY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "loss_fn = nn.L1Loss() # MAE loss with regression data\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "t_wwGuigCUGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "# Put the data on the target device\n",
        "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
        "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "  y_pred = model_2(X_train_regression)\n",
        "  loss = loss_fn(y_pred, y_train_regression)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Testing\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_2(X_test_regression)\n",
        "    test_loss = loss_fn(test_pred, y_test_regression)\n",
        "\n",
        "  # print out what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test loss: {test_loss:.5f}\")"
      ],
      "metadata": {
        "id": "exFShBSyCqUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on evaluation mode\n",
        "model_2.eval()\n",
        "\n",
        "# Make predictions (inference)\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_2(X_test_regression)\n",
        "\n",
        "# Plot data and predictions\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu())"
      ],
      "metadata": {
        "id": "YXu3lPZGEu05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The missing piece: non-linearity"
      ],
      "metadata": {
        "id": "dj7Wlpn5HRGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 6.1 Recreating non-linear data (red and blue circles)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "X, y = make_circles(n_samples=n_samples, noise=0.03, random_state=42)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "KP_8ssAvHlcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to tensors and then to train and test splits\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Turn data into tensors\n",
        "X = torch.from_numpy(X).type(torch.float)\n",
        "y = torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Split into train and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train[:5], y_train[:5]"
      ],
      "metadata": {
        "id": "UHsb_dS_IRT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a model with non-linearity\n",
        "\n",
        "* Linear = straigth line\n",
        "* Non-linear = non straight line\n",
        "\n",
        "Artificial neural networks are a large combination of linear (straight) and non-straight (non-linear) functions which are potentially able to find patterns in data."
      ],
      "metadata": {
        "id": "K_bc75RtJDlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a model with non-linear activation functions\n",
        "from torch import nn\n",
        "class CircleModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
        "    self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
        "    self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
        "    self.relu = nn.ReLU() # relu is a non-linear activation function\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Wherre should we put our non-linear activation functions\n",
        "    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
        "\n",
        "model_3 = CircleModelV2().to(device)\n",
        "model_3"
      ],
      "metadata": {
        "id": "lVpNH6RTJKOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "JoWPcOrFNBsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Training a model with non-linearity"
      ],
      "metadata": {
        "id": "72LevKnmNZtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Put all data on target device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Loop through data\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_3.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits = model_3(X_train).squeeze()\n",
        "  y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_logits, y_train)\n",
        "  acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_3.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_logits = model_3(X_test).squeeze()\n",
        "    test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "g-iN5F0tNPuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Evaluating a model trained with non-linear activation functions"
      ],
      "metadata": {
        "id": "Bi8lX9ohQCSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
        "\n",
        "y_preds[:10], y_test[:10]"
      ],
      "metadata": {
        "id": "ZsQ5A6MkQBoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_1, X_train, y_train) # model_1 linear\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_3, X_test, y_test) # model_3 non-linear"
      ],
      "metadata": {
        "id": "-__OXJexQdqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Replicating non-linear activation functions\n",
        "\n",
        "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own.\n",
        "\n",
        "And theses tools are linear & non-linear functions."
      ],
      "metadata": {
        "id": "llzDTfXJRaxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "A = torch.arange(-10, 10, 1).type(torch.float)\n",
        "A.dtype"
      ],
      "metadata": {
        "id": "sWnKmzn_RqV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the tensor\n",
        "plt.plot(A)"
      ],
      "metadata": {
        "id": "oKmES367R_gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(A))"
      ],
      "metadata": {
        "id": "dsnmemRKSFtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x: torch.Tensor) -> torch.Tensor:\n",
        "  return torch.maximum(torch.tensor(0), x) # inputs must be tensors"
      ],
      "metadata": {
        "id": "NawxDTVzSNK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ReLU activation function\n",
        "plt.plot(relu(A))"
      ],
      "metadata": {
        "id": "XcsoY6GVSinA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's do the same for Sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))"
      ],
      "metadata": {
        "id": "7XQ6ub-RSn0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.sigmoid(A))"
      ],
      "metadata": {
        "id": "eOHuqjOeS8b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sigmoid(A))"
      ],
      "metadata": {
        "id": "CNSINQGiTAs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Putting it all together with a multi-class classification problem\n",
        "\n",
        "* Binary classification = one thing or another (cat vs. dog, spam vs. not spam, fraud or not fraud)\n",
        "* Mutli-class classification = more than one thing or another (cat vs. dog vs. chicken)"
      ],
      "metadata": {
        "id": "BlXH4EraeHJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the hyperparameters for data creation\n",
        "NUM_CLASSES = 4\n",
        "NUM_FEATURES = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 1. Creat multi-class data\n",
        "X_blob, y_blob = make_blobs(n_samples=1000, n_features=NUM_FEATURES, centers=NUM_CLASSES, cluster_std=1.5, random_state=RANDOM_SEED)\n",
        "\n",
        "# 2. Turn data into tensors\n",
        "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
        "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
        "\n",
        "# 3. Split into train and test\n",
        "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "# 4. Plot data\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "hQ8eJMdOe8yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 Building a multi-class classification in PyTorch"
      ],
      "metadata": {
        "id": "iOI66e_qgxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "uW2KHaMxhQdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a multi-class classification model\n",
        "class BlobModel(nn.Module):\n",
        "  def __init__(self, input_features, output_features, hidden_units=8):\n",
        "    \"\"\"Initializes multi-class classification model\n",
        "\n",
        "    Args:\n",
        "      input_features (int): Number of input features to the model\n",
        "      output_features (int): Number of output features (number of output classes)\n",
        "      hidden_units (int): Number of hidden units between layers, default 8\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Example:\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_layer_stack = nn.Sequential(\n",
        "        nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "        # nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "        # nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_features)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_layer_stack(x)\n",
        "\n",
        "# Create an instance of BlobModel and send it to the target device\n",
        "model_4 = BlobModel(input_features=2, output_features=4, hidden_units=8).to(device)\n",
        "\n",
        "model_4"
      ],
      "metadata": {
        "id": "qNHvp6Uhhc5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_blob_train.shape, y_blob_train[:5]"
      ],
      "metadata": {
        "id": "gHaIN0xBk6D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.unique(y_blob_train)"
      ],
      "metadata": {
        "id": "7oBmz9Oql7kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Create a loss function and an optimizer for a multi-class classification model"
      ],
      "metadata": {
        "id": "sZnT_7udlisZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer for multi-class classification - optimizer updates our model parameters\n",
        "optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "6NSu-fHBlKPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Getting prediction probabilities for a multi-class PyTorch model\n",
        "\n",
        "In order to evaluate and train and test our model, we need to conver our nodel's outputs (logits) to prediction probabilities and then to prediction labels.\n",
        "\n",
        "Logits (raw outpuyt of the model) -> Pred probs (use `torch.softmax`) -> Pred labels (take the argmax of the prediction probabilities)"
      ],
      "metadata": {
        "id": "ePXwzdrxmVNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get some raw output of our model (logits)\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_4(X_blob_test.to(device))\n",
        "\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "ZPPte5OwmU4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_test[:10]"
      ],
      "metadata": {
        "id": "_b-WqLQjnjW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our model's logit outputs to prediction probabilities\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
        "print(y_logits[:5])\n",
        "print(y_pred_probs[:5])"
      ],
      "metadata": {
        "id": "zyMIHnCantiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our model's probabilities to prediction labels\n",
        "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "OKFqHTxEpC6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Create a training loop and testing loop for a multi-class PyTorch model"
      ],
      "metadata": {
        "id": "-6CytQKSpmOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the multi-class model to the data\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# Put data to the target device\n",
        "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
        "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
        "\n",
        "# Loop through data\n",
        "for epoch in range(epochs):\n",
        "  ### training\n",
        "  model_4.train()\n",
        "\n",
        "  y_logits = model_4(X_blob_train)\n",
        "  y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
        "\n",
        "  loss = loss_fn(y_logits, y_blob_train)\n",
        "  acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_4.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_logits = model_4(X_blob_test)\n",
        "    test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "\n",
        "    test_loss = loss_fn(test_logits, y_blob_test)\n",
        "    test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_pred)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "RWpDhvoBpl1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Making and evaluating predictions with a PyTorch multi-class model"
      ],
      "metadata": {
        "id": "a4fQthBjvV6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits = model_4(X_blob_test)\n",
        "\n",
        "# View the first 10 predictions\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "Tvj07JpHvagV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go from logits -> Precition probabilities\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
        "y_pred_probs[:10]"
      ],
      "metadata": {
        "id": "t9bUVCm6vwYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go from pred probs to pred labels\n",
        "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "jeHZAVOiv6zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_4, X_blob_train, y_blob_train) # model_1 linear\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_4, X_blob_test, y_blob_test) # model_3 non-linear"
      ],
      "metadata": {
        "id": "1RmzWkg2wOii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. A few more classification metrics.. (to evaluate our classification model)\n",
        "\n",
        "* Accurace - out of 100 samples, how many does our model get right?\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* Confusion matrix\n",
        "* Classification report\n"
      ],
      "metadata": {
        "id": "m6h6DSigw4Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "b8_rxHICyhV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Setup metrics\n",
        "torchmetrics_accuracy = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "# Calculate accuracy\n",
        "torchmetrics_accuracy(y_preds, y_blob_test)"
      ],
      "metadata": {
        "id": "QkY9UF0aGhOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}