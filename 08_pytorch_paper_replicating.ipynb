{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFGXDjaTpBOrVzBrz/61R9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Axeloooo/PyTorch/blob/main/08_pytorch_paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08. PyTorch Paper Replicating\n",
        "\n",
        "The goal of machine learning research paper replicating is: turn a ML research paper into usable code.\n",
        "\n",
        "In this notebook, we're going to be replicating the Vision Transformer (ViT) architecture/paper with PyTorch."
      ],
      "metadata": {
        "id": "AnB40N3ByyvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get Setup\n",
        "\n",
        "Let's import code we have previously written + required libraries"
      ],
      "metadata": {
        "id": "D84bd7VmzOQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0VexVdurgqc"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "GZPvPuSJz-39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "e0pSHnAI0IOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "The whole goal of what we're trying to do is replicate the ViT architecture for our FoodVision Mini problem.\n",
        "\n",
        "To do that, we need some data.\n",
        "\n",
        "Namely, pizza, steak and sushi images we've been using so far."
      ],
      "metadata": {
        "id": "6zb8BImX0S3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "RJRDJ04i0xjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "b_AB4LNo1HRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "YRUPUpIi1H5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n"
      ],
      "metadata": {
        "id": "5l0ivW3T1Q3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Prepare transforms for images"
      ],
      "metadata": {
        "id": "mHdunMpI8Ry1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ],
      "metadata": {
        "id": "AVsRDKxm2W28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.2 Turn images into `DataLoader`'s"
      ],
      "metadata": {
        "id": "1xKQqSVx8PTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "DP-ZWcBX3aff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualize a single image"
      ],
      "metadata": {
        "id": "gzBqKTEN3VMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "YgRjal8V3bSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "BrxfbgPE4DPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replicating ViT: Overview\n",
        "\n",
        "Looking at a whole machine learning research paper can be intimidating.\n",
        "\n",
        "So in order to make it more understandable we can break it down into smaller pieces.\n",
        "\n",
        "* **Inputs** - What goes into the model? (in our case, image tensors).\n",
        "* **Outputs** - What comes out of the model/layer/block? (in our case, we want the model to output image classification labels).\n",
        "* **Layers** - Takes an input, manipulate it with a function (for example could be self-attention).\n",
        "* **Blocks** - A collection of layers.\n",
        "* **Model** - A collection of blocks."
      ],
      "metadata": {
        "id": "nes7lhzN6G4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 ViT Overview: pieces of the puzzle\n",
        "\n",
        "* Figure 1: Visual of the architecture\n",
        "* Figure 2: Math equations which define the functions of each layer/block\n",
        "* Table 1/3: Different hyperparameters for the architecture/training."
      ],
      "metadata": {
        "id": "jZhKHpz6ENXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Figure 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "\n",
        "* Embedding = learnable representation (start with random numbers and improve over time)"
      ],
      "metadata": {
        "id": "Z2ucNgKUELN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Four Equations\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-four-equations.png)\n",
        "\n",
        "Section 3.1 describes the various equations:\n",
        "\n",
        "**Equation 1:**\n",
        "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "**Equation 1:**\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Equation 1\n",
        "x_input = [class_token, image_patch_1, image_patch_2, ... image_patch_N] + [class_token_pos, image_patch_1_pos, image_patch_2_pos, ... image_patch_N_pos]\n",
        "```\n",
        "\n",
        "**Equation 2&3:**\n",
        "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski \\& Auli, 2019).\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Equation 2\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "\n",
        "# Equation 3\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "\n",
        "**Equation 4:**\n",
        "Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches ( $\\mathbf{z}_0^0=\\mathbf{x}_{\\text {class }}$ ), whose state at the output of the Transformer encoder ( $\\mathbf{z}_L^0$ ) serves as the image representation $\\mathbf{y}$ (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{z}_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\n",
        "\n",
        "* MLP = multilayer perceptron = a neural network with X number of layers\n",
        "* MLP = one hidden layer at training time\n",
        "* MLP = single layer at fine-tunning time\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Equation 4\n",
        "y = Linear_layer(LN_layer(x_output_MLP_block))\n",
        "```"
      ],
      "metadata": {
        "id": "jV8u4y7K7Tey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Table 1\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-table-1.png)\n",
        "\n",
        "* ViT-Base, ViT-Large and ViT-Huge are all different sizes of the same model architecture.\n",
        "* ViT-B/16 = ViT-Base with image patch size 16x16\n",
        "* Layers - the number of transformer encoder layers.\n",
        "* Hidden size $D$ - the embedding size throughout the architecture.\n",
        "* MLP size - the number of hidden units/neurons\n",
        "* Heads - the number of multi-head self-attention\n"
      ],
      "metadata": {
        "id": "DO5-kw4tEFNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Equation 1: Split data into patches and creating the class, position and patch embedding\n",
        "\n",
        "Layers = input -> function -> output\n",
        "\n",
        "What's the input shape?\n",
        "\n",
        "What's the output shape?\n",
        "\n",
        "* Input shape: (224, 224, 3) -> single image -> (height, width, color channels)\n",
        "\n",
        "* Output shape:\n"
      ],
      "metadata": {
        "id": "jAG5m0UW_isA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Calculate input and output shapes by hand\n",
        "\n",
        "> **Equation 1:**\n",
        "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "> **Equation 1:**\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "\n",
        "* Input shape: ${H}\\times{W}\\times{C}$ (height x width x color channels)\n",
        "* Ouput shape: ${N}\\times({P^2}\\times{C})$\n",
        "* H = height\n",
        "* W = width\n",
        "* C = color channels\n",
        "* P = patch size\n",
        "* N = number of patches = ${H}\\times{W}/{P^2}$\n",
        "* D = constant latent vector size = embedding dimension"
      ],
      "metadata": {
        "id": "Vl-TVCnxCf7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example values\n",
        "height = 224 # H (\"The training resolution is 224.\")\n",
        "width = 224 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 16 # P\n",
        "\n",
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
      ],
      "metadata": {
        "id": "Y69G0jc5Dxq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape (this is the size of a single image)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
      ],
      "metadata": {
        "id": "Y3-206BADyov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Turning a single image into patches"
      ],
      "metadata": {
        "id": "uXvwr2m7D0cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "T-d0sz80D-UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ],
      "metadata": {
        "id": "Pu7gYns9Fm01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(\n",
        "    nrows=1,\n",
        "    ncols=img_size // patch_size, # one column for each patch\n",
        "    figsize=(num_patches, num_patches),\n",
        "    sharex=True,\n",
        "    sharey=True\n",
        ")\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "  axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
        "  axs[i].set_xlabel(i+1) # set the label\n",
        "  axs[i].set_xticks([])\n",
        "  axs[i].set_yticks([])"
      ],
      "metadata": {
        "id": "VvQtPhdxPt05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(\n",
        "    nrows=img_size // patch_size, # need int not float\n",
        "    ncols=img_size // patch_size,\n",
        "    figsize=(num_patches, num_patches),\n",
        "    sharex=True,\n",
        "    sharey=True\n",
        ")\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "  for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "\n",
        "    # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
        "    axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
        "                                        patch_width:patch_width+patch_size, # iterate through width\n",
        "                                        :]) # get all color channels\n",
        "\n",
        "    # Set up label information, remove the ticks for clarity and set labels to outside\n",
        "    axs[i, j].set_ylabel(\n",
        "        i+1,\n",
        "        rotation=\"horizontal\",\n",
        "        horizontalalignment=\"right\",\n",
        "        verticalalignment=\"center\"\n",
        "    )\n",
        "    axs[i, j].set_xlabel(j+1)\n",
        "    axs[i, j].set_xticks([])\n",
        "    axs[i, j].set_yticks([])\n",
        "    axs[i, j].label_outer()\n",
        "\n",
        "# Set a super title\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shuTs84qQ5Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Creating image patches and turning them into patch embeddings\n",
        "\n",
        "Perhaps we could create the image patches and image patch embeddings in a single step using `torch.nn.Cov2d()` and setting the kernel size and stride parameters to `patch_size`."
      ],
      "metadata": {
        "id": "M9A1hZu3Rxzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=16\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(\n",
        "    in_channels=3, # number of color channels\n",
        "    out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
        "    kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
        "    stride=patch_size,\n",
        "    padding=0\n",
        ")"
      ],
      "metadata": {
        "id": "5wusZeoVUq9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "EtQWAiIMVxVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ],
      "metadata": {
        "id": "2sWf--dFVsEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've oassed a single image to our `conv2d` layer, it's shpe is:\n",
        "\n",
        "```python\n",
        "torch.size([1, 7768, 14, 14]) # [batch_size, embedding_dim, feature_map_height, feature_map_width]\n",
        "```"
      ],
      "metadata": {
        "id": "PHNv3eVwWSMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random 5 convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "  image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
        "  axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
        "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
      ],
      "metadata": {
        "id": "Dj8diEbAWGtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ],
      "metadata": {
        "id": "ehCkOyrSYndD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Flattening the patch embedding with `torch.nn.Flatten()`\n",
        "\n",
        "Right now we've a series of convolutional feature maps (patch embeddings) that we want to flatten into a sequence of patch embeddings to satisfy the input criteria of the ViT Transformer Encoder."
      ],
      "metadata": {
        "id": "O-wEziemYwsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current tensor shape\n",
        "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "eDMDap-yZn-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create flatten layer\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ],
      "metadata": {
        "id": "Xp14Z8EpbhCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ],
      "metadata": {
        "id": "_HRN854jbkYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get flattened image patch embeddings in right shape\n",
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ],
      "metadata": {
        "id": "HXXj5TQYb-H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "Q9yV-7hwcArv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Turning the ViT patch embedding layer into a PyTorch module\n",
        "\n",
        "We want this module to do a few things:\n",
        "\n",
        "1. Create a class `PatchEmbedding`\n",
        "2. Initialize with appropriate hyperparameters, such as channels, embedding dimension, patch  size.\n",
        "3. Create a layer to turn an image into embedding patches using `nn.Conv2d()`\n",
        "4. Create a layer to flatten the feature maps of the output of the layer in 3.\n",
        "5. Define a `forward()` that defines the forward computation (e.g. pass through layer from 3 and 4).\n",
        "6. Make sure the output shape of the layer reflects the required output shape of the patch embeddings."
      ],
      "metadata": {
        "id": "RjbIoG72c37a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "  \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "  Args:\n",
        "    in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "    patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "    embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "  \"\"\"\n",
        "  # 2. Initialize the class with appropriate variables\n",
        "  def __init__(\n",
        "      self,\n",
        "      in_channels:int=3,\n",
        "      patch_size:int=16,\n",
        "      embedding_dim:int=768\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # 3. Create a layer to turn an image into patches\n",
        "    self.patcher = nn.Conv2d(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=embedding_dim,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "        padding=0\n",
        "    )\n",
        "\n",
        "    # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "    self.flatten = nn.Flatten(\n",
        "        start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "        end_dim=3\n",
        "    )\n",
        "\n",
        "    # 5. Define the forward method\n",
        "  def forward(self, x):\n",
        "    # Create assertion to check that inputs are the correct shape\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "    # Perform the forward pass\n",
        "    x_patched = self.patcher(x)\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "    # 6. Make sure the output shape has the right order\n",
        "    return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "Cme_-Q_Ke6mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(\n",
        "    in_channels=3,\n",
        "    patch_size=16,\n",
        "    embedding_dim=768\n",
        ")\n",
        "\n",
        "# Pass a single image through\n",
        "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
        "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
      ],
      "metadata": {
        "id": "xZAP6uKAgPr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Creating the class token embedding\n",
        "\n",
        "Want to: Prepend a learnable class token to the start of the patch embedding."
      ],
      "metadata": {
        "id": "Gec7FeTyHhbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the patch embedding and patch embedding shape\n",
        "print(patch_embedded_image)\n",
        "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "hxbeZJVPKq8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dimension = patch_embedded_image.shape[-1]\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(\n",
        "    torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "    requires_grad=True # make sure the embedding is learnable\n",
        ")\n",
        "\n",
        "# Show the first 10 examples of the class_token\n",
        "print(class_token[:, :, :10])\n",
        "\n",
        "# Print the class_token shape\n",
        "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "y8S3DMO5HqKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat(\n",
        "    (class_token, patch_embedded_image),\n",
        "    dim=1 # concat on first dimension\n",
        ")\n",
        "\n",
        "# Print the sequence of patch embeddings with the prepended class token embedding\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "1-x_oVjMKIg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7 Creating the position embedding\n",
        "\n",
        "Want to: Create a series of 1D learnable position embeddings and to add them to the sequence of patch embeddings."
      ],
      "metadata": {
        "id": "yc5HEXY9K5Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ],
      "metadata": {
        "id": "TxnGRkspLlZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(\n",
        "    torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "    requires_grad=True # make sure it's learnable\n",
        ")\n",
        "\n",
        "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
        "print(position_embedding[:, :10, :10])\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "sLtHiIXqMcpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "print(patch_and_position_embedding)\n",
        "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "8vQDPXwcNGxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.8 Putting it all together: from image to embedding\n",
        "\n",
        "We've written code to turn an image ina flattened sequence of patch embeddings."
      ],
      "metadata": {
        "id": "saGA_rb0QOGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Set patch size\n",
        "patch_size = 16\n",
        "\n",
        "# 2. Print shape of original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Get image tensor and add batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(\n",
        "    in_channels=3,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=768\n",
        ")\n",
        "\n",
        "# 5. Pass image through patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Create class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(\n",
        "    torch.ones(batch_size, 1, embedding_dimension),\n",
        "    requires_grad=True # make sure it's learnable\n",
        ")\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend class token embedding to patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(\n",
        "    torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "    requires_grad=True # make sure it's learnable\n",
        ")\n",
        "\n",
        "# 9. Add position embedding to patch embedding with class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "id": "dkDbdcQdQcjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Equation 2: Multihead Self-Attention (MSA block)\n",
        "\n",
        "* Multihead self-attention: which part of a sequence should pay the most attention to itself?\n",
        "  * In our case, we have a series of embedding image patches, which patch significantly relates to another patch.\n",
        "  * We want our neural network (ViT) to learn this relationship/representation.\n",
        "* LayerNorm = Layer normalization (LayerNorm) is a technique to normalize the distribution of intermidiate layers. It enables smoother gradients, faster training, and better generalization accuracy.\n",
        "  * Normalization = make everyhting have the same mean and standard deviation.\n",
        "  * In PyTorch LayerNorm normalized values over $D$ dimension, in our case the $D$ dimension is the embedding dimension."
      ],
      "metadata": {
        "id": "HlKcOW-VR9ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "  \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "  \"\"\"\n",
        "  # 2. Initialize the class with hyperparameters from Table 1\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "      num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "      attn_dropout:float=0 # doesn't look like the paper uses any dropout in MSABlocks\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    # 3. Create the Norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # 4. Create the Multi-Head Attention (MSA) layer\n",
        "    self.multihead_attn = nn.MultiheadAttention(\n",
        "        embed_dim=embedding_dim,\n",
        "        num_heads=num_heads,\n",
        "        dropout=attn_dropout,\n",
        "        batch_first=True # does our batch dimension come first?\n",
        "    )\n",
        "\n",
        "  # 5. Create a forward() method to pass the data through the layers\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    attn_output, _ = self.multihead_attn(\n",
        "        query=x, # query embeddings\n",
        "        key=x, # key embeddings\n",
        "        value=x, # value embeddings\n",
        "        need_weights=False # do we need the weights or just the layer outputs?\n",
        "    )\n",
        "    return attn_output"
      ],
      "metadata": {
        "id": "zmdF19wYE7P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
        "                                                             num_heads=12) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ],
      "metadata": {
        "id": "T2IQ4HmdGHfc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}