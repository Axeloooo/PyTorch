{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaEW9er4mrSXFHGjCa2IEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Axeloooo/PyTorch/blob/main/08_pytorch_paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08. PyTorch Paper Replicating\n",
        "\n",
        "The goal of machine learning research paper replicating is: turn a ML research paper into usable code.\n",
        "\n",
        "In this notebook, we're going to be replicating the Vision Transformer (ViT) architecture/paper with PyTorch."
      ],
      "metadata": {
        "id": "AnB40N3ByyvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get Setup\n",
        "\n",
        "Let's import code we have previously written + required libraries"
      ],
      "metadata": {
        "id": "D84bd7VmzOQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0VexVdurgqc"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "GZPvPuSJz-39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "e0pSHnAI0IOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get Data\n",
        "\n",
        "The whole goal of what we're trying to do is replicate the ViT architecture for our FoodVision Mini problem.\n",
        "\n",
        "To do that, we need some data.\n",
        "\n",
        "Namely, pizza, steak and sushi images we've been using so far."
      ],
      "metadata": {
        "id": "6zb8BImX0S3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "RJRDJ04i0xjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "b_AB4LNo1HRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "YRUPUpIi1H5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n"
      ],
      "metadata": {
        "id": "5l0ivW3T1Q3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Prepare transforms for images"
      ],
      "metadata": {
        "id": "mHdunMpI8Ry1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ],
      "metadata": {
        "id": "AVsRDKxm2W28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.2 Turn images into `DataLoader`'s"
      ],
      "metadata": {
        "id": "1xKQqSVx8PTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "DP-ZWcBX3aff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualize a single image"
      ],
      "metadata": {
        "id": "gzBqKTEN3VMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "YgRjal8V3bSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "BrxfbgPE4DPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Replicating ViT: Overview\n",
        "\n",
        "Looking at a whole machine learning research paper can be intimidating.\n",
        "\n",
        "So in order to make it more understandable we can break it down into smaller pieces.\n",
        "\n",
        "* **Inputs** - What goes into the model? (in our case, image tensors).\n",
        "* **Outputs** - What comes out of the model/layer/block? (in our case, we want the model to output image classification labels).\n",
        "* **Layers** - Takes an input, manipulate it with a function (for example could be self-attention).\n",
        "* **Blocks** - A collection of layers.\n",
        "* **Model** - A collection of blocks."
      ],
      "metadata": {
        "id": "nes7lhzN6G4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 ViT Overview: pieces of the puzzle\n",
        "\n",
        "* Figure 1: Visual of the architecture\n",
        "* Figure 2: Math equations which define the functions of each layer/block\n",
        "* Table 1/3: Different hyperparameters for the architecture/training.\n",
        "\n",
        "### Figure 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-figure-1-architecture-overview.png)\n",
        "\n",
        "* Embedding = learnable representation (start with random numbers and improve over time)\n",
        "\n",
        "### Four Equations\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-four-equations.png)\n",
        "\n",
        "### Table 1\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-table-1.png)"
      ],
      "metadata": {
        "id": "jV8u4y7K7Tey"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OO396JC6mUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}